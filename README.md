<h1 align="center">
  CHeaT&nbsp;ğŸ›¡ï¸â€” Cloak â€¢ Honey â€¢ Trap
</h1>

<p align="center">
  <a href="#-tool-quick-start">Quick&nbsp;Start</a> â€¢
  <a href="#-repository-layout">Repo&nbsp;Layout</a> â€¢
  <a href="#-paper">Paper</a> â€¢
  <a href="#-datasets">Datasets</a> â€¢
  <a href="#%EF%B8%8F-ctf-machines)">CTF&nbspMachines</a> â€¢
  <a href="#-token-landmines">Token&nbsp;Landmines</a> â€¢
  <a href="#-demo-notebook">Demo&nbsp;Notebook</a> â€¢
  <a href="#-citation">Citation</a>
</p>

---

## ğŸŒŸ Project Overview
CHeaT (Cloakâ€“Honeyâ€“Trap) is a research toolkit that **defends networks against autonomous LLM-powered pentesting agents** by:

1. **Cloaking** sensitive data with misdirection  
2. Planting **Honey** tokens to detect & fingerprint LLM agents  
3. Setting **Traps** that stall or crash abusive agents  

All techniques are described in our USENIX Security â€™25 white-paper and implemented in a light-weight CLI.

---

## ğŸš€ Tool Quick Start

> **TL;DR**

```bash
# clone repo & enter tool folder
git clone https://github.com/your-user/CHeaT.git
cd CHeaT

# optional: create venv
python3 -m venv .venv && source .venv/bin/activate

# install (pure-stdlib -> nothing to pull)
pip install -e .

# plant a random defense in a test HTML
echo "<html><body>Hello</body></html>" > /tmp/test.html
cheat --action plant --details '{
  "assettype": "web_file",
  "file_path": "/tmp/test.html",
  "technique": "random",
}'
````

| Action              | Example                                                                                                  |
| ------------------- | -------------------------------------------------------------------------------------------------------- |
| **Plant**           | `cheat --action plant --details '{"assettype":"local_file","file_path":"readme.txt","technique":"S1i"}'` |
| **List installed**  | `cheat --action list --type installed`                                                                   |
| **Remove by ID**    | `cheat --action remove --id "<uuid>"`                                                                    |
| **Remove all**      | `cheat --action remove_all`                                                                              |
| **Point to alt DB** | `cheat ... --database /path/to/db`                                                                       |

See [`cheat/README.md`](cheat/README.md) for full CLI docs.

---

## ğŸ“‚ Repository Layout

```
CHeaT/
â”œâ”€ cheat/               â† Python package (tool)
â”‚   â”œâ”€ database/        â† default JSON techniques & templates
â”‚   â””â”€ ...
â”œâ”€ datasets/            â† datasets used in the paper evaluations
â”œâ”€ ctf-machines/        â† ready-to-run vulnerable VMs
â”œâ”€ token-landmines/     â† unicode landmines
â”œâ”€ demo-notebook/       â† Jupyter walkthrough & sandbox
â”œâ”€ Whitepaper.pdf       â† full academic paper
â””â”€ README.md            â† you are here
```

---

## ğŸ“œ Paper

The full methodology, threat model, and evaluation are in **â€œCloak, Honey, Trap: Proactive Defenses Against LLM Agents.â€**
Read the PDF or check the arXiv link in the paper header for details.
Key numbers:

* 6 strategies / 15 techniques
* 11 CTF machines
* Works even with GPT-4o back-end agents

---

## ğŸ“Š Datasets

Directory **`datasets/`** collects:

---

## ğŸ–¥ï¸ CTF Machines

Ready-to-import OVA VMs used in the paperâ€™s evaluation:

* `UbuntuX`, `VulBox`, `Shocker`, â€¦
* Each folder contains **`README.md`** with flag locations & solution.

---

## ğŸ’£ Token Landmines

`token-landmines/` provides per-model rare-token sequences that induce hallucinations or context collapse.
Use them in **Trap T4.4** or study robustness across models.

---

## ğŸ“’ Demo Notebook

Launch the Jupyter notebook in `demo-notebook/` to:

* Step through the white-paper experiments
* Mix-and-match techniques live
* Sandbox to play in

---

## ğŸ“„ License

MIT Â© 2025 Ayzenshteyn â€¢ Weiss â€¢ Mirsky

---

## ğŸ¤ Citation

For citations:

```bibtex
@inproceedings{Ayzenshteyn2025CHeaT,
  title={{CHeaT}: Cloak, Honey, Trap â€“ Proactive Defenses Against LLM Agents},
  author={Daniel Ayzenshteyn and Roy Weiss and Yisroel Mirsky},
  booktitle={USENIX Security},
  year={2025}
}
```

Happy trapping! ğŸ•¸ï¸
